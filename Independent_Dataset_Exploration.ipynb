{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asanth7/Minority-USMedia-Representation/blob/main/Independent_Dataset_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "harEHYtPbSLG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP1bNGVWCeS2"
      },
      "outputs": [],
      "source": [
        "!pip install goose3 mediacloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vaO8UM3A5IcE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import mediacloud.api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import date, datetime\n",
        "# from newspaper import Article\n",
        "from goose3 import Goose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fWhPfquijytK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure article dataset is loaded in current working directory and change name as appropriate\n",
        "\n",
        "dataset_name = \"mc_articles\"\n",
        "\n",
        "mc_articles_text = pd.read_csv(dataset_name + \".csv\")\n",
        "mc_articles_text.attrs['name'] = dataset_name\n",
        "print(len(mc_articles_text))\n",
        "mc_articles_text.head()"
      ],
      "metadata": {
        "id": "ZQ8q4-4NbtFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "  # All NaN/null values in 'text' column --> error in scraping\n",
        "  null_count = df['text'].isnull().sum().sum()\n",
        "  assert null_count == df.isnull().sum().sum()\n",
        "  print('Number of null values:', null_count)\n",
        "\n",
        "  df.dropna(inplace = True)\n",
        "  df.reset_index(drop = True, inplace = True)\n",
        "  df.drop(['url'], axis = 1, inplace = True)\n",
        "  print(len(df))\n",
        "  if \"publish_date\" in df.columns:\n",
        "    df['publish_date'] = pd.to_datetime(df['publish_date']).dt.date\n",
        "  print(df.info(verbose=True))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "h7NxeMm-VCIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_articles_text = preprocess_df(mc_articles_text)"
      ],
      "metadata": {
        "id": "C7KPQNqXVh0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from datetime import date\n",
        "\n",
        "def subset_articles(end_date, name, start_date=date(2022, 1, 1), df=mc_articles_text):\n",
        "  df_dates = df[(df['publish_date'] >= start_date) & (df['publish_date'] <= end_date)]\n",
        "  subset_df = df_dates.drop(['publish_date'], axis = 1)\n",
        "  subset_df.reset_index(drop = True, inplace = True)\n",
        "  subset_df.attrs['name'] = name\n",
        "  return subset_df"
      ],
      "metadata": {
        "id": "x7VOVugpETNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can optionally subset data to specific years, if desired.\n",
        "\n",
        "# mc_articles_2022 = subset_articles(end_date = date(2022, 12, 31), name = \"mc_articles_2022\")\n",
        "# mc_articles_2023 = subset_articles(end_date = date(2023, 12, 31), start_date = date(2023, 1, 1), name = \"mc_articles_2023\")\n",
        "# mc_articles_2024 = subset_articles(end_date = date.today(), start_date = date(2024, 1, 1), name = \"mc_articles_2024\")"
      ],
      "metadata": {
        "id": "1B5ks_BXI0By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_publishers_articles(df):\n",
        "\n",
        "  file_ext = df.attrs['name'].split(\"_\")[-2]\n",
        "\n",
        "  top_15_publishers = df['media_name'].value_counts().head(15)\n",
        "  top_15_publishers.plot(kind='bar')\n",
        "  plt.title(\"Top 15 Publishers by Article Count\")\n",
        "  plt.xlabel(\"Publisher\")\n",
        "  plt.ylabel(\"Number of Articles\")\n",
        "  plt.xticks(rotation=45, ha = \"right\")\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"top_15_publishers_{file_ext}.png\")\n",
        "  plt.show()\n",
        "\n",
        "  if 'publish_date' in df.columns:\n",
        "    df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
        "    articles_daily = df.groupby(df['publish_date'].dt.date).size()\n",
        "    articles_daily.plot(kind='line')\n",
        "    plt.title(\"Number of Articles per Publish Date\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Article Count\")\n",
        "    plt.xticks(rotation=45, ha = \"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"articles_daily_{file_ext}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZMZLY9MDVnbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View bar chart of articles per publisher and graph of daily article frequency from data\n",
        "\n",
        "plot_publishers_articles(mc_articles_text)"
      ],
      "metadata": {
        "id": "LqJxDLL3WopS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Non-Negative Matrix Factorization"
      ],
      "metadata": {
        "id": "wsoy8gSibm-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF, MiniBatchNMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "mZK-L1twhcDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 2, max_features = 15000, stop_words = 'english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(mc_articles_text['text'])"
      ],
      "metadata": {
        "id": "KISIaogxkx98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with NMF hyperparameters\n",
        "\n",
        "num_components = 10\n",
        "max_iterations = 7500\n",
        "l1_ratio = 0.5\n",
        "\n",
        "nmf_model = NMF(\n",
        "    n_components = num_components,\n",
        "    random_state = 2024,\n",
        "    init = \"nndsvda\",\n",
        "    max_iter = max_iterations,\n",
        "    #beta_loss = \"kullback-leibler\",\n",
        "    solver = \"mu\",\n",
        "    alpha_W = 0.00005, # 0.00005\n",
        "    l1_ratio = 0.5, # 0.5 best\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "nmf_model.fit(tfidf)"
      ],
      "metadata": {
        "id": "yZTD9jfomTc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly adapted from the sklearn documentation\n",
        "\n",
        "def plot_top_words(model_obj, features, num_top_words, loss_used=None, type=\"NMF\"):\n",
        "  fig, axes = plt.subplots(2, 5, figsize = (30, 15), sharex = True)\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  topics_and_features = {}\n",
        "  for topic_index, topic in enumerate(model_obj.components_):\n",
        "    top_feature_indices = topic.argsort()[-num_top_words:]\n",
        "    top_features = [features[x] for x in top_feature_indices]\n",
        "    weights = topic[top_feature_indices]\n",
        "\n",
        "    topic_bar_axis = axes[topic_index]\n",
        "    topic_bar_axis.barh(top_features, weights, height=0.7)\n",
        "    topics_and_features[topic_index] = top_features\n",
        "    topic_bar_axis.set_title(f\"Topic {topic_index + 1}\", fontdict = {\"fontsize\": 30})\n",
        "    topic_bar_axis.tick_params(axis = \"both\", which = \"major\", labelsize = 20)\n",
        "    for i in [\"top\", \"right\", \"left\"]:\n",
        "      topic_bar_axis.spines[i].set_visible(False)\n",
        "  #f\"Topics in NMF Model with {loss_used} Loss\"\n",
        "\n",
        "  if (type == \"NMF\"):\n",
        "    sep = loss_used.split(\"-\")\n",
        "    if (len(sep) > 1):\n",
        "      loss_used = \"-\".join([sep[0].capitalize(), sep[1].capitalize()])\n",
        "    fig.suptitle(f\"Topics in NMF Model with {loss_used} Loss\", fontsize = 40)\n",
        "  else:\n",
        "    fig.suptitle(\"Topics in LDA Model\", fontsize = 40)\n",
        "  plt.subplots_adjust(bottom = 0.05, wspace = 0.90, hspace = 0.3)\n",
        "  #plt.savefig(\"NMF_top_words.png\")\n",
        "  plt.show()\n",
        "\n",
        "  return topics_and_features"
      ],
      "metadata": {
        "id": "eed6XKfpC-GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_words = 20\n",
        "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
        "topics_and_features = plot_top_words(nmf_model, tfidf_features, num_top_words, nmf_model.beta_loss.capitalize())"
      ],
      "metadata": {
        "id": "1ltt8mq9-XkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_components = nmf_model.components_\n",
        "total_weight = nmf_components.sum()\n",
        "feature_frequencies = 100 * nmf_components.sum(axis=0) / total_weight\n",
        "\n",
        "feature_freq_dict = {tfidf_features[i]: feature_frequencies[i] for i in range(len(tfidf_features))}\n",
        "\n",
        "all_sorted_features = sorted(feature_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(all_sorted_features[:20])"
      ],
      "metadata": {
        "id": "n3zBwIxVRyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_freq_df = pd.DataFrame(feature_freq_dict.items(), columns = ['Feature', 'Relative Weight']).sort_values(['Relative Weight'], ascending = False)\n",
        "#feature_freq_df['Num_Articles'] = round(len(mc_articles_text) * feature_freq_df['Relative Weight'] / 100)\n",
        "feature_freq_df.reset_index(drop = True, inplace = True)\n",
        "#feature_freq_df = feature_freq_df[feature_freq_df['Num_Articles'] > 0]\n",
        "feature_freq_df.head(15)"
      ],
      "metadata": {
        "id": "8HCHwIj4j89M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "feature_occurrences = np.sum(tfidf, axis = 0)\n",
        "words_summed_tfidf = {tfidf_features[i]: feature_occurrences[0, i] for i in range(len(tfidf_features))}\n",
        "sorted_words = sorted(words_summed_tfidf.items(), key = lambda x: x[1], reverse=True)\n",
        "words_summed_tfidf_sorted = {word: tfidf_score for word, tfidf_score in sorted_words}\n",
        "print(list(words_summed_tfidf_sorted)[:15])\n",
        "\n",
        "total_tfidf = feature_occurrences.sum()\n",
        "word_perc_tfidf = {word: (tfidf_score / total_tfidf) * 100 for word, tfidf_score in words_summed_tfidf_sorted.items()}\n",
        "list(word_perc_tfidf)[:15]"
      ],
      "metadata": {
        "id": "jAsggnyRns-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, max_features = 15000, stop_words = 'english')\n",
        "tf = tf_vectorizer.fit_transform(mc_articles_text['text'])"
      ],
      "metadata": {
        "id": "ESgqA4yvmKGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_occurences = (tf.toarray() > 0).astype(int).sum(axis = 0)\n",
        "\n",
        "word_occurence_dict = dict(zip(tfidf_features, word_occurences))\n",
        "sorted_word_occurences = sorted(word_occurence_dict.items(), key = lambda x: x[1], reverse=True)\n",
        "word_occurence_dict = {word: count for word, count in sorted_word_occurences if count > 0}\n",
        "list(word_occurence_dict)[:10]"
      ],
      "metadata": {
        "id": "FDgVUeAqRWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_freq_df['Num_Articles'] = feature_freq_df['Feature'].map(word_occurence_dict)\n",
        "feature_freq_df['tf-idf_score'] = feature_freq_df['Feature'].map(words_summed_tfidf_sorted)\n",
        "feature_freq_df['Percent_Article_Occurence'] = 100 * round((feature_freq_df['Num_Articles'] / len(mc_articles_text)), 4)\n",
        "feature_freq_df.head(15)"
      ],
      "metadata": {
        "id": "YXQwQteLUYe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at frequencies/in different time periods/intervals (i.e. in 2022, 2023, 2024)"
      ],
      "metadata": {
        "id": "ehdI4yKsTpw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "xtv5zhlDWcCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis tmtoolkit plotly"
      ],
      "metadata": {
        "id": "QnGpgy7BjO9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
      ],
      "metadata": {
        "id": "u4HjWtVYjHeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook(local=True)\n",
        "\n",
        "def run_LDA_analysis(df, text_type='text', max_iterations=50, num_top_words=20):\n",
        "\n",
        "  print(f\"Running model on {df.attrs['name']}\")\n",
        "\n",
        "  tf_vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, max_features = 10000, stop_words = 'english')\n",
        "  tf = tf_vectorizer.fit_transform(df[text_type])\n",
        "\n",
        "  lda_model = LatentDirichletAllocation(\n",
        "      n_components = 15,\n",
        "      max_iter = max_iterations,\n",
        "      random_state = 2024,\n",
        "      learning_method = \"online\",\n",
        "      verbose = 1\n",
        "  )\n",
        "\n",
        "  lda_model.fit(tf)\n",
        "\n",
        "  topic_word_distribution = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
        "  # print(topic_word_distribution.shape)\n",
        "\n",
        "  analyzer_func = tf_vectorizer.build_analyzer()\n",
        "  tokenized_articles = [analyzer_func(article) for article in df[text_type]]\n",
        "  # print(len(tokenized_articles))\n",
        "\n",
        "  sorted_vocabulary = sorted(tf_vectorizer.vocabulary_.items(), key = lambda x: x[1])\n",
        "  words = [pair[0] for pair in sorted_vocabulary]\n",
        "\n",
        "  # print(np.array(words).shape)\n",
        "\n",
        "  print(\"Calculating coherence score...\")\n",
        "  coherence_score = metric_coherence_gensim(measure=\"u_mass\", topic_word_distrib=topic_word_distribution,\n",
        "                                            dtm=tf, vocab=np.array(words),\n",
        "                                            texts=tokenized_articles)\n",
        "  dict_coherence = {i: coherence_score[i - 1] for i in range(1, len(coherence_score) + 1)}\n",
        "  sorted_coherence = sorted(dict_coherence.items(), key = lambda x: x[1])\n",
        "  print(sorted_coherence)\n",
        "\n",
        "  topic_dists_per_doc = lda_model.transform(tf) # _vectorizer.transform(df[text_type])\n",
        "\n",
        "  prepared_data = pyLDAvis.lda_model.prepare(lda_model, tf, tf_vectorizer)\n",
        "  pyLDAvis.display(prepared_data)\n",
        "\n",
        "  return lda_model, tf_vectorizer, tf, topic_dists_per_doc"
      ],
      "metadata": {
        "id": "2d6tSzS3lu8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_articles_text.attrs['name'] = \"mc_articles_text\"\n",
        "all_years_LDA, all_years_vectorizer, tf_all, topic_dists_all = run_LDA_analysis(mc_articles_text, text_type=\"text\")"
      ],
      "metadata": {
        "id": "GQ0AmtS6BCLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Plot interactive LDA intertopic distance visualization\n",
        "\n",
        "def plot_save_LDA(model, tf_file, vect, title):\n",
        "  prepared_data = pyLDAvis.lda_model.prepare(model, tf_file, vect)\n",
        "  pyLDAvis.save_html(prepared_data, title)\n",
        "\n",
        "  return display(HTML(title))"
      ],
      "metadata": {
        "id": "WN8nipYZYvH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_save_LDA(all_years_LDA, tf_all, all_years_vectorizer, \"LDA_15topics_all_years_titles.html\")"
      ],
      "metadata": {
        "id": "Gt1gK3Nk-8zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example number-topic mapping to match LDA numerical topics with semantic meaning\n",
        "\n",
        "# nums_to_topics_all = {\n",
        "#     2: \"Film\",\n",
        "#     (6, 11): \"American/Asian Culture + Heritage/Identity\",\n",
        "#     (4, 13): \"Proliferation of Violence/Hate + Minority Insecurity\",\n",
        "#     (1, 12): \"Foreign Policy, Govt, Politics\",\n",
        "#     (8, 15): \"Affirmative Action and Education\",\n",
        "#     9: \"Social Culture/Events\",\n",
        "#     10: \"Health, Data and Science\",\n",
        "#     7: \"Economy, Businesses, and Tech\"\n",
        "# }"
      ],
      "metadata": {
        "id": "2Ql8OtBGznAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_rename_df(df, nums_to_topics):\n",
        "  for item in nums_to_topics.items():\n",
        "    if isinstance(item[0], int):\n",
        "      print(\"Renaming \" + f\"Topic_{item[0]}\" + \" to \" + str(item[1]))\n",
        "      df.rename(columns = {f\"Topic_{item[0]}\": item[1]}, inplace = True)\n",
        "    else:\n",
        "      cols = [f\"Topic_{i}\" for i in item[0]]\n",
        "      df[item[1]] = sum([df[x] for x in cols])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "HzD6rdPI5lPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_dist_by_publisher(df, publisher, nums_to_topics):\n",
        "  labels = list(nums_to_topics.values())\n",
        "  labels.append(\"Counts\")\n",
        "  sizes = list(df.loc[df['media_name'] == publisher][labels].values[0])\n",
        "  article_count = sizes[len(labels) - 1]\n",
        "  labels.pop()\n",
        "  sizes.pop()\n",
        "  sizes.append(1 - sum(sizes))\n",
        "  labels.append(\"Other\")\n",
        "\n",
        "  year = \"20\" + df.attrs['name'][-2:]\n",
        "\n",
        "  fig1, ax1 = plt.subplots(figsize=(8, 5))\n",
        "  wedges, text, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "  plt.title(f\"{publisher} Topic Distribution {year} (n = {round(article_count)})\")\n",
        "  plt.savefig(f\"{publisher}_topic_dist_{year}.png\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "j3o2DWLIzpH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_publishers(model, df, topic_distribution, id2topic, publisher_list): #, publisher_list = list(mc_articles_text['media_name'].value_counts().index)[:3]\n",
        "  print(\"Topic distribution shape: \" + str(topic_distribution.shape))\n",
        "\n",
        "  topic_cols = [f\"Topic_{i}\" for i in range (1, model.components_.shape[0] + 1)]\n",
        "  for idx, name in enumerate(topic_cols):\n",
        "    df[name] = topic_distribution[:, idx]\n",
        "\n",
        "  publisher_groups = df.groupby(['media_name'])[topic_cols].mean()\n",
        "  publisher_groups.insert(0, 'media_name', publisher_groups.index)\n",
        "  publisher_groups['Counts'] = df['media_name'].value_counts(sort = False)\n",
        "  publisher_groups = publisher_groups.sort_values(by = \"Counts\", ascending = False)\n",
        "  publisher_groups.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  publisher_groups = clean_rename_df(publisher_groups, id2topic)\n",
        "\n",
        "  for publisher in publisher_list:\n",
        "    topic_dist_by_publisher(publisher_groups, publisher, id2topic)"
      ],
      "metadata": {
        "id": "rF5Q3AXLMCXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function calls to visualize pie-chart topic distributions by publisher\n",
        "\n",
        "# analyze_publishers(mc_articles_text, topic_dists_all, nums_to_topics_all)\n",
        "# analyze_publishers(model22, mc_articles_2022, dists22, nums_topics_22)\n",
        "# analyze_publishers(model23, mc_articles_2023, dists23, nums_topics_23)\n",
        "# analyze_publishers(model24, mc_articles_2024, dists24, nums_topics_24, publisher_list=list(mc_articles_2024['media_name'].value_counts().index)[:3])\n",
        "# analyze_publishers(model_black, mc_articles_black_24, dists_black, num_topics_black_24, publisher_list=list(mc_articles_black_24['media_name'].value_counts().index)[:3])\n",
        "# analyze_publishers(model_hl, mc_articles_hl_24, dists_hl, num_topics_hl_24)"
      ],
      "metadata": {
        "id": "ZE0Dnh3nNzFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "harEHYtPbSLG"
      ],
      "authorship_tag": "ABX9TyNq11pPkKuCkxWVeKtzxVMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}